{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "\n",
    "* The goal of this notebook is to show examples of how to create item and user features. \n",
    "* Mab2Rec is _independent_ of item, user, and interaction data used in recommendations and assumes that input data is created before building recommenders. \n",
    "* Sample input is given in `data/` which includes user features in `features_user.csv` and item features in `features_item.csv`.\n",
    "* This notebook shows examples of how to create user or item features from **structured**, **unstructured**, and **sequential** data.\n",
    "* In addition to techniques covered here, and you can utilize any other source to create your input data. \n",
    "* An overview of these libraries is [presented at All Things Open 2021](https://www.youtube.com/watch?v=54d_YUalvOA)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Structured Data via Selective](#Structured-Data-via-Selective)\n",
    "2. [Unstructured Data via TextWiser](#Unstructured-Data-via-TextWiser)\n",
    "3. [Sequential Data via Seq2Pat](#Sequential-Data-via-Seq2Pat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structured Data via Selective\n",
    "\n",
    "* The most common data source is structured, tabular data. \n",
    "* In recommenders, the typical usage of structured data is to represent **user features**.\n",
    "* When there are many user features to consider, feature selection can decide which features to include in the user context.\n",
    "* For feature selection, you can leverage [Selective](https://github.com/fidelity/selective).\n",
    "* Selective provides an easy-to-use API for supervised and unsupervised feature selection methods.\n",
    "* In unsupervised fashion, given a set of users, important features can be identified according to variance, correlation and statistical measures. \n",
    "* In supervised fashion, given a set of users _and_ the interaction label (e.g., click on _any item_), important features can be identified according to a linear or non-linear model. \n",
    "* Let's install the library and explore a quick start example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install Selective\n",
    "!pip install -q selective;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import Selective and SelectionMethod\n",
    "from sklearn.datasets import load_boston\n",
    "from feature.utils import get_data_label\n",
    "from feature.selector import Selective, SelectionMethod\n",
    "\n",
    "# Data\n",
    "data, label = get_data_label(load_boston())\n",
    "\n",
    "# Feature selectors from simple to more complex\n",
    "selector = Selective(SelectionMethod.Variance(threshold=0.0))\n",
    "selector = Selective(SelectionMethod.Correlation(threshold=0.5, method=\"pearson\"))\n",
    "selector = Selective(SelectionMethod.Statistical(num_features=3, method=\"anova\"))\n",
    "selector = Selective(SelectionMethod.Linear(num_features=3, regularization=\"none\"))\n",
    "selector = Selective(SelectionMethod.TreeBased(num_features=3))\n",
    "\n",
    "# Feature reduction\n",
    "subset = selector.fit_transform(data, label)\n",
    "print(\"Reduction:\", list(subset.columns))\n",
    "print(\"Scores:\", list(selector.get_absolute_scores()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* In this example, we show 5 different `selector` methods. \n",
    "* Any selection approach can be used to `fit_transform` the dataset. \n",
    "* A more robust approach is to apply different selectors, and then to select feautures that are deemed important by several selectors. \n",
    "* It is even better to repeat this within cross-validation to make sure the selection is stable. \n",
    "* Selective offers a benchmarking utility to achieve this. \n",
    "* See [Selective Benchmarking](https://github.com/fidelity/selective#benchmarking)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unstructured Data via TextWiser\n",
    "\n",
    "* Unstructured data is another common data source utilizing text, audio, and video features.   \n",
    "* In recommenders, the typical usage of unstructured data is to represent **item features**.\n",
    "* Unstructured data should first be featurized before consumption in recommenders.  \n",
    "* For text data, you can leverage [TextWiser](https://github.com/fidelity/textwiser) to create text embeddings of item representations.\n",
    "* TextWiser ([AAAI'21](https://ojs.aaai.org/index.php/AAAI/article/view/17814)) provides an easy-to-use API for a rich set of text featurization methods and their transformation while taking advantage of state-of-the-art pretrained NLP models.\n",
    "* Let's install the library and explore a quick start example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install TextWiser\n",
    "!pip install -q textwiser;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Conceptually, TextWiser is composed of an Embedding, potentially with a pretrained model,\n",
    "# that can be chained into zero or more Transformations\n",
    "from textwiser import TextWiser, Embedding, Transformation, WordOptions, PoolOptions\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Data\n",
    "documents = [\"Some document\", \"More documents. Including multi-sentence documents.\"]\n",
    "\n",
    "# Model: TFIDF `min_df` parameter gets passed to sklearn automatically\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1))\n",
    "\n",
    "# Model: TFIDF followed with an NMF + SVD\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1), [Transformation.NMF(n_components=30), Transformation.SVD(n_components=10)])\n",
    "\n",
    "# Model: Word2Vec with no pretraining that learns from the input data\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained=None), Transformation.Pool(pool_option=PoolOptions.min))\n",
    "\n",
    "# Model: BERT with the pretrained bert embedding\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.bert), Transformation.Pool(pool_option=PoolOptions.first))\n",
    "\n",
    "# Features\n",
    "vecs = emb.fit_transform(documents)\n",
    "\n",
    "print(documents)\n",
    "print(vecs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* In this example, we show different embeddings from simple `TFIDF` to more complex `BERT`. \n",
    "* Notice how the `Embedding` can be followed with one more transformation operations, such as `NMF` or `SVD`. \n",
    "* In general, the `Transformation` reduces the dimensionality of the text representation to create succint embeddings. \n",
    "* Running `fit_transform` on the documents return the embedding of each document. \n",
    "* Checkout different word options, pre-trained models, and other transformations.\n",
    "* See the rich list of [TextWiser Embeddings](https://github.com/fidelity/textwiser#available-embeddings)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequential Data via Seq2Pat\n",
    "\n",
    "* For most applications, structured data for user features via Selective and unstructured data for item features via TextWiser will suffice.\n",
    "* In applications where events occur in sequences, time horizon can serve as another source of information to build advanced recommenders.\n",
    "* We suggest starting simple and consider such sequential data to improve upon initial results once an end-to-end benchmarking is completed.\n",
    "* In recommenders, the typical usage of sequential data is to capture the behaviour of a user over time as part of **user features**. \n",
    "* For sequential data, you can leverage [Seq2Pat](https://github.com/fidelity/seq2pat).\n",
    "* Seq2Pat ([AAAI'22](https://aaai.org/Conferences/AAAI-22/)) provides an easy-to-use API for frequent pattern mining in sequential datasets. \n",
    "* We first find frequent patterns with desired properties.\n",
    "* Then, we represent each user with a one-hot vector denoting the existence of frequent patterns in user's sequential behaviour. This one-hot features can be used to augment user representation.\n",
    "* Let's install the library and explore a quick start example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install Seq2Pat\n",
    "!pip install -q seq2pat;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Example to show how to find frequent sequential patterns\n",
    "# from a given sequence database subject to constraints\n",
    "# and generate one-hot encodings as the features of each user\n",
    "from sequential.seq2pat import Seq2Pat, Attribute\n",
    "from sequential.dpm import get_one_hot_encodings\n",
    "\n",
    "# Sequences database with 3 sequences\n",
    "sequences = [[\"A\", \"A\", \"B\", \"A\", \"D\"],\n",
    "             [\"C\", \"B\", \"A\"],\n",
    "             [\"C\", \"A\", \"C\", \"D\"]]\n",
    "\n",
    "# Prices of each item in sequences\n",
    "values = [[5, 5, 3, 8, 2],\n",
    "          [1, 3, 3],\n",
    "          [4, 5, 2, 1]]\n",
    "\n",
    "# Seq2Pat over 3 sequences\n",
    "seq2pat = Seq2Pat(sequences=sequences)\n",
    "\n",
    "# Price attribute corresponding to each item\n",
    "price = Attribute(values=values)\n",
    "\n",
    "# Average price constraint\n",
    "seq2pat.add_constraint(3 <= price.average() <= 4)\n",
    "\n",
    "# Patterns that occur at least twice (A-D)\n",
    "patterns = seq2pat.get_patterns(min_frequency=2)\n",
    "print(\"Frequent Patterns: \", patterns)\n",
    "\n",
    "# Encoding of each user with respect to frequent patterns (A-D)\n",
    "encodings = get_one_hot_encodings(sequences, patterns)\n",
    "print(\"Encodings:\\n\", encodings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* In this example, we have 3 users with certain sequential events, e.g. page visits in-order. \n",
    "* Notice, how the length of each sequence in the sequence database can be different.\n",
    "* We can mine for frequent patterns in this sequence database. The `min_frequency` threshold denotes the minimum number of occurrence of a pattern to be considered frequent.\n",
    "* More importantly, we consider **attributes** that correspond to each sequential event. Here, the price of the item in each page visit.\n",
    "* Then, we add **constraints** to reason about attributes. Here, the average price to be between 3 and 4.\n",
    "* Pattern mining operates on the sequence database and seeks frequent patterns that meet the minimum frequency threshold and satisfy constraints.\n",
    "* See other [Seq2Pat Constraints](https://github.com/fidelity/seq2pat/blob/master/notebooks/usage_example.ipynb) for .\n",
    "* Finally, we create one-hot encodings of each user with respect to frequent pattern (A-D). The one-hot vector indicates the existence of such pattern in the user's sequence.\n",
    "* These one-hot encodings of users can be used as input features for the recommendation model.\n",
    "* In this example, we mined patterns for the entire population. A more advanced approach is to mine patterns that distinguish between the positive vs. negative behavior, e.g., differentiate users who engaged with a certain content from those who do not.\n",
    "* To that end, see [Dichotomic Pattern Mining (DPM)](https://github.com/fidelity/seq2pat/blob/master/notebooks/dichotomic_pattern_mining.ipynb) from [AAAI'22](https://arxiv.org/abs/2201.09178) that exploits the dichotomy of such behavior to patterns unique to each cohort."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}