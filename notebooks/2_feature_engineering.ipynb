{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install libraries to create user and item features\n",
    "!pip install -q selective;\n",
    "!pip install -q textwiser;\n",
    "!pip install -q seq2pat;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "* The goal of this notebook is to show examples of how to create item and user features. \n",
    "* Mab2Rec is _independent_ of item, user, and interaction data used in recommendations and assumes that input data is created before building recommenders. \n",
    "* Sample input is given in `data/` which includes user features in `features_user.csv` and item features in `features_item.csv`.\n",
    "* This notebook shows examples of how to create user or item features from **structured**, **unstructured**, and **sequential** data.\n",
    "* In addition to techniques covered here, and you can utilize any other source to create your input data. \n",
    "* An overview of these libraries is [presented at All Things Open 2021](https://www.youtube.com/watch?v=54d_YUalvOA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Structured Data via Selective](#Structured-Data-via-Selective)\n",
    "2. [Unstructured Data via TextWiser](#Unstructured-Data-via-TextWiser)\n",
    "3. [Sequential Data via Seq2Pat](#Sequential-Data-via-Seq2Pat)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Data via Selective\n",
    "\n",
    "* The most common data source is structured, tabular data. \n",
    "* In recommenders, the typical usage of structured data is to represent **user features**.\n",
    "* When there are many user features to consider, feature selection can decide which features to include in the user context.\n",
    "* For feature selection, you can leverage [Selective](https://github.com/fidelity/selective).\n",
    "* Selective provides an easy-to-use API for supervised and unsupervised feature selection methods.\n",
    "* In unsupervised fashion, given a set of users, important features can be identified according to variance, correlation and statistical measures. \n",
    "* In supervised fashion, given a set of users _and_ the interaction label (e.g., click on _any item_), important features can be identified according to a linear or non-linear model. \n",
    "* Let's explore a quick start example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction: ['RM', 'DIS', 'LSTAT']\n",
      "Scores: [0.03785112185484013, 0.0009506038523461075, 0.005597120928343709, 0.0006564025010623736, 0.02400336475058042, 0.4385965510576283, 0.01419565112974908, 0.06486214843307006, 0.00430519230815954, 0.014455710682486108, 0.016132316878544155, 0.0107710325703614, 0.3676227830528288]\n"
     ]
    }
   ],
   "source": [
    "# Import Selective and SelectionMethod\n",
    "from sklearn.datasets import load_boston\n",
    "from feature.utils import get_data_label\n",
    "from feature.selector import Selective, SelectionMethod\n",
    "\n",
    "# Data\n",
    "data, label = get_data_label(load_boston())\n",
    "\n",
    "# Feature selectors from simple to more complex\n",
    "selector = Selective(SelectionMethod.Variance(threshold=0.0))\n",
    "selector = Selective(SelectionMethod.Correlation(threshold=0.5, method=\"pearson\"))\n",
    "selector = Selective(SelectionMethod.Statistical(num_features=3, method=\"anova\"))\n",
    "selector = Selective(SelectionMethod.Linear(num_features=3, regularization=\"none\"))\n",
    "selector = Selective(SelectionMethod.TreeBased(num_features=3))\n",
    "\n",
    "# Feature reduction\n",
    "subset = selector.fit_transform(data, label)\n",
    "print(\"Reduction:\", list(subset.columns))\n",
    "print(\"Scores:\", list(selector.get_absolute_scores()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example, we show 5 different `selector` methods. \n",
    "* Any selection approach can be used to `fit_transform` the dataset. \n",
    "* A more robust approach is to apply different selectors, and then to select feautures that are deemed important by several selectors. \n",
    "* It is even better to repeat this within cross-validation to make sure the selection is stable. \n",
    "* Selective offers a benchmarking utility to achieve this. \n",
    "* See [Selective Benchmarking](https://github.com/fidelity/selective#benchmarking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Data via TextWiser\n",
    "\n",
    "* Unstructured data is another common data source utilizing text, audio, and video features.   \n",
    "* In recommenders, the typical usage of unstructured data is to represent **item features**.\n",
    "* Unstructured data should first be featurized before consumption in recommenders.  \n",
    "* For text data, you can leverage [TextWiser](https://github.com/fidelity/textwiser) to create text embeddings of item representations.\n",
    "* TextWiser ([AAAI'21](https://ojs.aaai.org/index.php/AAAI/article/view/17814)) provides an easy-to-use API for a rich set of text featurization methods and their transformation while taking advantage of state-of-the-art pretrained NLP models.\n",
    "* Let's explore a quick start example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Some document', 'More documents. Including multi-sentence documents.']\n",
      "[[-0.19813849  0.25826398 -0.16164416 ... -0.18344447 -0.04831381\n",
      "   0.27247164]\n",
      " [-0.38283885 -0.03924329 -0.10620081 ... -0.25401732  0.21510349\n",
      "   0.4452555 ]]\n"
     ]
    }
   ],
   "source": [
    "# Conceptually, TextWiser is composed of an Embedding, potentially with a pretrained model,\n",
    "# that can be chained into zero or more Transformations\n",
    "from textwiser import TextWiser, Embedding, Transformation, WordOptions, PoolOptions\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Data\n",
    "documents = [\"Some document\", \"More documents. Including multi-sentence documents.\"]\n",
    "\n",
    "# Model: TFIDF `min_df` parameter gets passed to sklearn automatically\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1))\n",
    "\n",
    "# Model: TFIDF followed with an NMF + SVD\n",
    "emb = TextWiser(Embedding.TfIdf(min_df=1), [Transformation.NMF(n_components=30), Transformation.SVD(n_components=10)])\n",
    "\n",
    "# Model: Word2Vec with no pretraining that learns from the input data\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.word2vec, pretrained=None), Transformation.Pool(pool_option=PoolOptions.min))\n",
    "\n",
    "# Model: BERT with the pretrained bert embedding\n",
    "emb = TextWiser(Embedding.Word(word_option=WordOptions.bert), Transformation.Pool(pool_option=PoolOptions.first))\n",
    "\n",
    "# Features\n",
    "vecs = emb.fit_transform(documents)\n",
    "\n",
    "print(documents)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example, we show different embeddings from simple `TFIDF` to more complex `BERT`. \n",
    "* Notice how the `Embedding` can be followed with one more transformation operations, such as `NMF` or `SVD`. \n",
    "* In general, the `Transformation` reduces the dimensionality of the text representation to create succint embeddings. \n",
    "* Running `fit_transform` on the documents return the embedding of each document. \n",
    "* Checkout different word options, pre-trained models, and other transformations.\n",
    "* See the rich list of [TextWiser Embeddings](https://github.com/fidelity/textwiser#available-embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Data via Seq2Pat\n",
    "\n",
    "* Time horizon is another source of information to build advanced recommenders.\n",
    "* In recommenders, the typical usage of sequential data is to capture the behaviour of a user over time as part of **user features**. \n",
    "* For sequential data, you can leverage [Seq2Pat](https://github.com/fidelity/seq2pat).\n",
    "* Seq2Pat ([AAAI'22](https://aaai.org/Conferences/AAAI-22/)) provides an easy-to-use API for frequent pattern mining in sequential datasets. \n",
    "* First, we find frequent patterns. Then, each user is transformed into a one-hot vector denoting the existence of frequent patterns in their sequential behaviour. This representation can be used as their user features.\n",
    "* Let's explore a quick start example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Patterns:  [['A', 'D', 2]]\n",
      "Encodings:\n",
      "           sequence  feature_0\n",
      "0  [A, A, B, A, D]          1\n",
      "1        [C, B, A]          0\n",
      "2     [C, A, C, D]          1\n"
     ]
    }
   ],
   "source": [
    "# Example to show how to find frequent sequential patterns\n",
    "# from a given sequence database subject to constraints\n",
    "# and generate one-hot encodings as the features of each user\n",
    "from sequential.seq2pat import Seq2Pat, Attribute\n",
    "from sequential.dpm import get_one_hot_encodings\n",
    "\n",
    "# Sequences and attributes data\n",
    "sequences = [[\"A\", \"A\", \"B\", \"A\", \"D\"],\n",
    "             [\"C\", \"B\", \"A\"],\n",
    "             [\"C\", \"A\", \"C\", \"D\"]]\n",
    "\n",
    "values = [[5, 5, 3, 8, 2],\n",
    "          [1, 3, 3],\n",
    "          [4, 5, 2, 1]]\n",
    "\n",
    "# Seq2Pat over 3 sequences\n",
    "seq2pat = Seq2Pat(sequences=sequences)\n",
    "\n",
    "# Price attribute corresponding to each item\n",
    "price = Attribute(values=values)\n",
    "\n",
    "# Average price constraint\n",
    "seq2pat.add_constraint(3 <= price.average() <= 4)\n",
    "\n",
    "# Patterns that occur at least twice (A-D)\n",
    "patterns = seq2pat.get_patterns(min_frequency=2)\n",
    "print(\"Frequent Patterns: \", patterns)\n",
    "\n",
    "# Encodings that are generated using the minded pattern (A-D)\n",
    "encodings = get_one_hot_encodings(sequences, patterns)\n",
    "print(\"Encodings:\\n\", encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example, we have 3 users with certain sequential events, e.g. page visits in-order. \n",
    "* Notice, how the length of each sequence in the sequence database is different.\n",
    "* We can mine for frequent patterns in this sequence database while setting a `min_frequency` threshold to denote the minimum number of occurence of a pattern to be considered frequent. \n",
    "* More importantly, we consider **attributes** that correspond to each sequential event. For example, the price of the item in each page visit. \n",
    "* Then, we add **constraints** to reason about attributes, here, the average price to be between 3 and 4. \n",
    "* Pattern mining operates on the sequence database and seeks frequent patterns while satisfying constraints and the minimum frequency threshold. \n",
    "* See [Seq2Pat Constraints](https://github.com/fidelity/seq2pat/blob/master/notebooks/usage_example.ipynb).\n",
    "* Finally, we create one-hot encodings of each user with the mined pattern (A-D), indicating the existence of such pattern in a sequence.\n",
    "* See [Dichotomic Pattern Mining](https://github.com/fidelity/seq2pat/blob/master/notebooks/dichotomic_pattern_mining.ipynb) (DPM) as a integrator technology between Sequential Pattern Mining and downstream modeling tasks via Seq2Pat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
